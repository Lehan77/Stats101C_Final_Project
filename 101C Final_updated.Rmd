---
title: "STATS 101C PRJKT"
output:
  pdf_document: default
  html_document: default
---
```{r}
library(nnet)
library(randomForest)
library(caret)
library(e1071)
library(MASS)
library(class)
```

# 1. Data preprocessing

```{r}
# remove id and index
data <- read.csv("Data_Final.csv", na.strings="")
data <- data[,c(-1,-2,-3,-9,-10,-16)]
data <- na.omit(data)
```


```{r}
clean_review = function(x){
  x <- gsub("\n"," ",x) # remove new lines
  x <- gsub(","," ",x) # remove ","
  x <- gsub("!","",x) # remove "!"
  x <- gsub("\\.","",x) # remove "."
  x <- gsub("\\(","",x) # remove "("
  x <- gsub("\\)","",x) # remove ")"
  x <- tolower(x) # transform all capitals to lower letters
  x
}
data$Review <- clean_review(data$Review)
```


# 2. Build sentiment dictionary

```{r}
pos_word <- read.table("positive-words.txt")$V1
neg_word <- read.table("negative-words.txt")$V1
all_word <- c(pos_word,neg_word)

get_review_dict <- function(x){
  x <- strsplit(x," ")[[1]]
  out <- logical(0)
  pos_num <- 0
  neg_num <- 0
  for(word in x){ # count positive words
    if(word %in% pos_word){
      pos_num <- pos_num + 1
    }
    else if(word %in% neg_word){ # count positive words
      neg_num <- neg_num +1
    }
  }
  c(pos_num,neg_num,length(x)) # positive words amount, negative words amount, all words amount
}

for(i in 1:dim(data)[1]){
  out <- get_review_dict(data[i,"Review"])
  if(i == 1){
    all_dict <- out
  }
  else{
    all_dict <- cbind(all_dict, out)
  }
}
scores <- as.data.frame(t(all_dict))
colnames(scores) <- c("pos","neg","length") # create new columns
data <- cbind(data,scores) 
data <- data[, -5] # remove the review column
```

Here we created a new column named "relative_sentiment". It is calculated by using ( the number of postive words - the number of negative words) / length of the review. It represents both the sentiment direction, either positive or negative, and the intensity of the sentiment. If the value == 0, then it represents neutral sentiment; if the value is high positive, it represents a extremely satisfied review; if the value is low positive, it means the sentiment is so-so positive. It combines the value 'pos', 'neg', and 'length' into one.
```{r}
data$relative_sentiment <- (data$pos - data$neg) / data$length 
head(data)
```


# 4.Select features using random forest

```{r}
data$Star <- as.factor(data$Star) # transform into factor so we can build random forest
rf <- randomForest(Star~., data = data, ntree = 500, importance = T) # build random forest with n = 500
plot(rf,main = "Error Rate vs Number of Trees") # draw a plot to see if number of trees differs significantly, and the result shows that n = 100 is good enough
rf <- randomForest(Star~., data = data, ntree = 100, importance = T) # through testing, it takes a lot more time to run random forest with 500 trees, so we choose n = 100 to reduce computational expense
imp <- data.frame(importance(rf)) # calculate importance so we can decide which features to use
imp <- imp[order(imp$MeanDecreaseAccuracy, decreasing = TRUE), ] # sort by Mean Decrease Accuracy
print(imp)

varImpPlot(rf, type = 1, scale = F) # ...or we can just use the graph to see which predictors are the most important ones, much simpler
```

From the plot, we observe a significant drop on the mean decrease accuracy, there, we conclude it is a good idea to select the top three important features, which are 'relative_sentiment', 'Users_Ave_Star', 'Bus_Ave_Star'.


```{r}
select_feature <- rownames(imp)[1:3] # select the top 3 most important predictors
data <- data[,c("Star", select_feature)]
```

Write out the selected data:

```{r}
# new data with relative_sentiment, users_ave_star, bus_ave_star
write.csv(data,"Data_select.csv")
```

# 5. Build model and do validation

```{r}
data <- read.csv("Data_select.csv")
data <- data[, -1] #remove the index column
head(data) # one response variable with three explanatory variables
```

### Split data into training and testing dataset

```{r}
set.seed(605558924)
i <- 1:dim(data)[1]
# Generate a random sample.
i.train <- sample(i, length(i) * 0.7, replace = F)
X.train <- data[i.train,-1]
Y.train <- data[i.train,1]  
X.test <- data[-i.train,-1]
Y.test <- data[-i.train,1]
```


## KNN 

#### Finding the best K with the highest accuracy

```{r}
accuracy <- numeric()
for(i in 1:40) {
  knn <- knn(train = X.train, test = X.test, cl = Y.train, k = i)
  table.knn <- table(Y.test, knn)
  accuracy[i] <- sum(table.knn[row(table.knn) == col(table.knn)]) / dim(X.test)[1]
  print(paste('k = ', i, ' accuracy = ', accuracy[i]))
}
```


```{r}
# accuracy plot
plot(accuracy, type = "b", xlab = "K-value", ylab = "Accuracy Level")
```

####  Fitting the KNN with the optimal k-value on the training dataset

```{r}
m.knn <- knn(train = X.train, test = X.test, cl = Y.train, k = 34)
```


#### Model Evaluation

```{r}
# confusion matrix
confusionMatrix(table(m.knn, Y.test), mode = "everything")
```


## Multinomial Logistic Model

```{r}
m.logistic <- multinom(Star~., data = data[i.train, ])
y_pred_logistic <- predict(m.logistic, data[-i.train, ], type = "class")
confusionMatrix(table(y_pred_logistic, Y.test), mode = "everything")
```

## LDA

```{r}
m.lda <- lda(Star~., data = data[i.train, ])
y_pred_lda <- predict(m.lda, data[-i.train, ])$class
confusionMatrix(table(y_pred_lda, Y.test), mode = "everything")
```

## QDA

```{r}
m.qda <- qda(Star~., data = data[i.train, ])
y_pred_qda <- predict(m.qda, data[-i.train, ])$class
confusionMatrix(table(y_pred_qda, Y.test), mode = "everything")
```







